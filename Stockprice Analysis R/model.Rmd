---
title: "Analyzing Stock Price Prediction with a Focus on Timeliness"
author: "Xiaodong(David) Zheng  ï½œ  Yuxi Chai"
date: '2024-05-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(xgboost)
library(dplyr)
library(SHAPforxgboost)
library(iml)
```
<u>**Problem Definition:**</u>\
The ability to predict stock prices is a complex yet profitable task in the financial markets. With the increasing availability of financial data and the advancements in machine learning techniques, there has been a growing interest in implementing sophisticated models for stock price prediction. However, implementing said prediction comes with great challenges due to the dynamic nature of financial markets. This project aims to explore various aspects of stock price prediction model, including data pre-processing, feature creation and selection, and model selection, with a focus on how time dynamics affect the predictions.\

<u>**Proposed Idea:**</u>\
- Collect stock price data from online archive sources.\
- Feature engineering. Calculate additional two predictors, SMA and RSI, based on collected stock data.\
- Predict stock price by modeling Closing Price againest all the predictor data from the previous day.\
- Analyze feature importance to identify which input variables (features) have the most influence on the predictive model's output.\
- Analyze model residue in a time series manner to evaluate model accuracy and identify any systematic errors in predicting stock price movements.\

<u>**Experiment:**</u>\
<u>*Data Collection:*</u>\
We gathered stock price and volume data from API through twelvedata. The collected raw data type includes Open price, Close price, High price, Low price, Trading Volume, MACD (Moving Average Convergence/Divergence), and EMA (Exponential Moving Average), all from Apple stock spamming the previous 1000 days.\
```{r}
# load data collected from twelvedata
stock_data <- read.csv("../Data Collection/out.csv")
```

<u>*Additional Feature Engineering:*</u>\
Two additional feature is calculated from the price data, SMA (simple moving average) \[ \text{SMA}_t = \frac{X_t + X_{t-1} + X_{t-2} + \ldots + X_{t-n+1}}{n} \] with $X$ being stock price and $n$ being the intended calculation time period, and RSI (Relative Strength Index) \[ \text{RSI} = 100 - \frac{100}{1 + \frac{\text{Average Gain}}{\text{Average Loss}}} \] .\
```{r}
# Feature Engineering: Calculate technical indicators
stock_data$SMA <- SMA(Cl(stock_data), n = 20)  # 20-day Simple Moving Average
stock_data$RSI <- RSI(Cl(stock_data), n = 14)  # 14-day Relative Strength Index
```

<u>*Setup Model:*</u>\
We choose to setup the prediction using XGBoost (Extreme Gradient Boosting). XGBoost is efficient with large datasets due to its parallel and distributed computing capabilities. Although in this particular project the size of the dataset is manageable, in real world scenario the dataset would be massive. Another reason is that XGBoost performs well with datasets that have a large number of features, making it suitable for our application with many predictors. Finally XGBooost provides feature importance scores, which can help us in feature selection and understanding the importance of different features in the model.
```{r}
# Prepare dataset for modeling
stock_data <- na.omit(stock_data)
X <- stock_data[, 2:ncol(stock_data)]
y <- lead(Cl(stock_data), default = NA)  # Lagged dependent variable (next day's closing price)

# Split data into training and testing sets
trainSize <- floor(0.8 * nrow(X))
X_train <- X[1:trainSize, ]
X_test <- X[(trainSize + 1):nrow(X), ]
y_train <- y[1:trainSize]
y_test <- y[(trainSize + 1):length(y)]

# Create model
data_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
params <- list(
  objective = "reg:squarederror",
  eval_metric = "mae"
)

xgboost_model <- xgb.train(params = params, data = data_train, nrounds = 100)
```

<u>*Feature Importance Analysis:*</u>\
Through feature analysis, we can see that, quite intuitively, the Open Price plays the most important role in predicting the Closing Price for next day since intuitively this is the baseline for the prediction. Similar idea applies to the High Price, Low Price and previous day Close Price. Interestingly we see that EMA and Trading Volume also play a rather significant role in the prediction.
```{r}
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap_values$mean_shap_score
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
```

<u>*Residue Analysis:*</u>\
Analyzing the prediction using MAE (mean absolute error).
```{r}
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mae <- mean(na.omit(abs(y_pred - y_test)))
cat("This XGBoost model yields an MAE of:", mae)
```
We next formed a more detailed residue analysis with a focus on the time dynamics. To achieve this we plot the residue data with respect to the time index. From the graph we can infer two conclusions:\
- a. There seems to be oscillation happening in the residue data. This could induce further research into ways to isolate the oscillations into s combination of a varieties of trend or seasonality, thus allowing the model to make more accurate predictions.\
- b. The prediction accuracy does seems to degrade significantly as time reaches a certain point, in this case around the 75 days mark. It would be interesting to expand and repeat the experiment on data from different stocks and see how each performs.
```{r}
# Plot residuals over time
residuals <- y_test - y_pred
plot(residuals, type = "h", col = "blue", main = "Residuals", xlab = "Time Index", ylab = "Residuals")
abline(h = 0, col = "red")  # Add a baseline at zero to highlight errors
```

Additional code and dataset can be found on https://github.com/David-XZ/Stockprice-Analysis-Project.git