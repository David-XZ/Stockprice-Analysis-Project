y_test <- y[(trainSize + 1):length(y)]
View(X)
# Prepare dataset for modeling
stock_data <- na.omit(stock_data)
X <- stock_data[, 2:ncol(stock_data)]
y <- lead(Cl(stock_data), default = NA)  # Lagged dependent variable (next day's closing price)
# Split data into training and testing sets
trainSize <- floor(0.8 * nrow(X))
X_train <- X[1:trainSize, ]
X_test <- X[(trainSize + 1):nrow(X), ]
y_train <- y[1:trainSize]
y_test <- y[(trainSize + 1):length(y)]
View(X)
data_train <- xgb.DMatrix(data = as.matrix(as.numeric(X_train)), label = y_train)
data_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
params <- list(
objective = "reg:squarederror",
eval_metric = "rmse"
)
# create model
xgboost_model <- xgb.train(params = params, data = data_train, nrounds = 100)
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
importance_matrix <- xgb.importance(model = xgboost_model)
print(importance_matrix)
# analyze feature importance
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
# analyze feature importance
importance_matrix <- xgb.importance(model = xgboost_model)
print(importance_matrix)
# analyze feature importance
importance_matrix <- xgb.importance(model = xgboost_model)
print(importance_matrix)
install.packages("SHAPforxgboost")
install.packages("SHAPforxgboost")
install.packages("SHAPforxgboost")
knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(xgboost)
library(dplyr)
library(SHAPforxgboost)
library(SHAPforxgboost)
install.packages("SHAPforxgboost")
install.packages("SHAPforxgboost")
install.packages("SHAPforxgboost")
install.packages("SHAPforxgboost")
install.packages("SHAPforxgboost")
knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(xgboost)
library(dplyr)
library(SHAPforxgboost)
# analyze feature importance
importance_matrix <- xgb.importance(model = xgboost_model)
print(importance_matrix)
shap_values <- shap.values(xgb_model = xgboost_model, X_train = X_train)
mod <- xgboost::xgboost(data = dataX, label = as.matrix(dataXY_df[[y_var]]),
params = params, nrounds = 200,
verbose = FALSE,
early_stopping_rounds = 8)
mod <- xgboost::xgboost(data = X_train, label = Y_train,
params = params, nrounds = 200,
verbose = FALSE,
early_stopping_rounds = 8)
mod <- xgboost::xgboost(data = X_train, label = y_train,
params = params, nrounds = 200,
verbose = FALSE,
early_stopping_rounds = 8)
mod <- xgboost::xgboost(data = as.matrix(X_train), label = y_train,
params = params, nrounds = 200,
verbose = FALSE,
early_stopping_rounds = 8)
shap_values <- shap.values(xgb_model = xgboost_model, X_train = X_train)
mod <- xgboost::xgboost(data = as.matrix(X_train), label = y_train,
params = params, nrounds = 200,
verbose = FALSE,
early_stopping_rounds = 8)
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap_values$mean_shap_score
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap_values$mean_shap_score
shap_long <- shap.prep(xgb_model = mod, X_train = as.matrix(X_train))
shap.plot.summary.wrap2(shap_values$shap_score, dataX)
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap_values$mean_shap_score
shap_long <- shap.prep(xgb_model = mod, X_train = as.matrix(X_train))
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap_long <- shap.prep(xgb_model = mod, X_train = as.matrix(X_train))
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap.plot.summary.wrap(shap_values$shap_score, as.matrix(X_train))
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
rmse
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
y_pred - y_test
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
mean((y_pred - y_test)^2)
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mse <- mean((y_pred - y_test)^2)
rmse <- sqrt(mse)
(y_pred - y_test)^2
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mse <- mean(as.numeric((y_pred - y_test)^2))
rmse <- sqrt(mse)
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mse <- mean(as.numeric((y_pred - y_test)^2))
rmse <- sqrt(mse)
as.numeric((y_pred - y_test)^2)
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mse <- mean(as.numeric((y_pred - y_test)^2))
rmse <- sqrt(mse)
na.omit((y_pred - y_test)^2)
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mse <- mean(na.omit((y_pred - y_test)^2))
rmse <- sqrt(mse)
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mae <- mean(na.omit(abs(y_pred - y_test)))
mae
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mae <- mean(na.omit(abs(y_pred - y_test)))
mae
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mae <- mean(na.omit(abs(y_pred - y_test)))
cat("This XGBoost model yields an MAE of:", mae)
summarise(y)
summarise(na.omit(y))
summarise(as.numeric(na.omit(y)))
y
(na.omit(y))
summarise(na.omit(y))
summery(na.omit(y))
summary(na.omit(y))
knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(xgboost)
library(dplyr)
library(SHAPforxgboost)
# load data collected from twelvedata
stock_data <- read.csv("../Data Collection/out.csv")
# Feature Engineering: Calculate technical indicators
stock_data$SMA <- SMA(Cl(stock_data), n = 20)  # 20-day Simple Moving Average
stock_data$RSI <- RSI(Cl(stock_data), n = 14)  # 14-day Relative Strength Index
# Prepare dataset for modeling
stock_data <- na.omit(stock_data)
X <- stock_data[, 2:ncol(stock_data)]
y <- lead(Cl(stock_data), default = NA)  # Lagged dependent variable (next day's closing price)
# Split data into training and testing sets
trainSize <- floor(0.8 * nrow(X))
X_train <- X[1:trainSize, ]
X_test <- X[(trainSize + 1):nrow(X), ]
y_train <- y[1:trainSize]
y_test <- y[(trainSize + 1):length(y)]
# Create model
data_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
params <- list(
objective = "reg:squarederror",
eval_metric = "rmse"
)
xgboost_model <- xgb.train(params = params, data = data_train, nrounds = 100)
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mae <- mean(na.omit(abs(y_pred - y_test)))
cat("This XGBoost model yields an MAE of:", mae)
# Plot residuals over time
residuals <- y_test - y_pred
plot(residuals, type = "h", col = "blue", main = "Residuals", xlab = "Index", ylab = "Residuals")
abline(h = 0, col = "red")  # Add a baseline at zero to highlight errors
# Plot residuals over time
residuals <- y_test - y_pred
plot(residuals, type = "h", col = "blue", main = "Residuals", xlab = "Time Index", ylab = "Residuals")
abline(h = 0, col = "red")  # Add a baseline at zero to highlight errors
knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(xgboost)
library(dplyr)
library(SHAPforxgboost)
library(iml)
# Create a data frame with the feature data used for training
feature_data <- as.data.frame(X_train)
# Create a prediction function that converts input data frames to DMatrix and predicts with the model
predict_xgb <- function(model, newdata) {
dmatrix <- xgb.DMatrix(as.matrix(newdata))
predict(model, dmatrix)
}
# Create the Predictor object using this prediction function
predictor <- Predictor$new(
model = xgboost_model,
data = feature_data,
y = y_train,
predict.fun = function(x) predict_xgb(xgboost_model, x)
)
sample_index <- 1
sample_instance <- feature_data[sample_index, , drop = FALSE]  # Ensure it's still a data frame
# Compute Shapley values for the selected instance
shapley <- Shapley$new(predictor = predictor, x.interest = sample_instance)
# Create a data frame with the feature data used for training
feature_data <- as.data.frame(X_train)
# Create a prediction function that converts input data frames to DMatrix and predicts with the model
predict_xgb <- function(model, newdata) {
dmatrix <- xgb.DMatrix(as.matrix(newdata))
predict(model, dmatrix)
}
# Create the Predictor object using this prediction function
predictor <- Predictor$new(
model = xgboost_model,
data = feature_data,
y = y_train,
predict.fun = function(x) predict_xgb(xgboost_model, x)
)
sample_index <- 1
sample_instance <- feature_data[sample_index, , drop = FALSE]  # Ensure it's still a data frame
# Compute Shapley values for the selected instance
shapley <- Shapley$new(predictor = predictor, x.interest = sample_instance)
# Prepare dataset for modeling
stock_data <- na.omit(stock_data)
X <- stock_data[, 2:ncol(stock_data)]
y <- lead(Cl(stock_data), default = NA)  # Lagged dependent variable (next day's closing price)
# Split data into training and testing sets
trainSize <- floor(0.8 * nrow(X))
X_train <- X[1:trainSize, ]
X_test <- X[(trainSize + 1):nrow(X), ]
y_train <- y[1:trainSize]
y_test <- y[(trainSize + 1):length(y)]
# Create model
data_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
params <- list(
objective = "reg:squarederror",
eval_metric = "rmse"
)
xgboost_model <- xgb.train(params = params, data = data_train, nrounds = 100)
# Create a data frame with the feature data used for training
feature_data <- as.data.frame(X_train)
# Create a prediction function that converts input data frames to DMatrix and predicts with the model
predict_xgb <- function(model, newdata) {
dmatrix <- xgb.DMatrix(as.matrix(newdata))
predict(model, dmatrix)
}
# Create the Predictor object using this prediction function
predictor <- Predictor$new(
model = xgboost_model,
data = feature_data,
y = y_train,
predict.fun = function(x) predict_xgb(xgboost_model, x)
)
sample_index <- 1
sample_instance <- feature_data[sample_index, , drop = FALSE]  # Ensure it's still a data frame
# Compute Shapley values for the selected instance
shapley <- Shapley$new(predictor = predictor, x.interest = sample_instance)
knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(xgboost)
library(dplyr)
library(SHAPforxgboost)
library(iml)
# load data collected from twelvedata
stock_data <- read.csv("../Data Collection/out.csv")
# Feature Engineering: Calculate technical indicators
stock_data$SMA <- SMA(Cl(stock_data), n = 20)  # 20-day Simple Moving Average
stock_data$RSI <- RSI(Cl(stock_data), n = 14)  # 14-day Relative Strength Index
# Prepare dataset for modeling
stock_data <- na.omit(stock_data)
X <- stock_data[, 2:ncol(stock_data)]
y <- lead(Cl(stock_data), default = NA)  # Lagged dependent variable (next day's closing price)
# Split data into training and testing sets
trainSize <- floor(0.8 * nrow(X))
X_train <- X[1:trainSize, ]
X_test <- X[(trainSize + 1):nrow(X), ]
y_train <- y[1:trainSize]
y_test <- y[(trainSize + 1):length(y)]
# Create model
data_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
params <- list(
objective = "reg:squarederror",
eval_metric = "rmse"
)
xgboost_model <- xgb.train(params = params, data = data_train, nrounds = 100)
# Create a data frame with the feature data used for training
feature_data <- as.data.frame(X_train)
# Create a prediction function that converts input data frames to DMatrix and predicts with the model
predict_xgb <- function(model, newdata) {
dmatrix <- xgb.DMatrix(as.matrix(newdata))
predict(model, dmatrix)
}
# Create the Predictor object using this prediction function
predictor <- Predictor$new(
model = xgboost_model,
data = feature_data,
y = y_train,
predict.fun = function(x) predict_xgb(xgboost_model, x)
)
sample_index <- 1
sample_instance <- feature_data[sample_index, , drop = FALSE]  # Ensure it's still a data frame
# Compute Shapley values for the selected instance
shapley <- Shapley$new(predictor = predictor, x.interest = sample_instance)
knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(xgboost)
library(dplyr)
library(SHAPforxgboost)
library(iml)
# load data collected from twelvedata
stock_data <- read.csv("../Data Collection/out.csv")
# Feature Engineering: Calculate technical indicators
stock_data$SMA <- SMA(Cl(stock_data), n = 20)  # 20-day Simple Moving Average
stock_data$RSI <- RSI(Cl(stock_data), n = 14)  # 14-day Relative Strength Index
# Prepare dataset for modeling
stock_data <- na.omit(stock_data)
X <- stock_data[, 2:ncol(stock_data)]
y <- lead(Cl(stock_data), default = NA)  # Lagged dependent variable (next day's closing price)
# Split data into training and testing sets
trainSize <- floor(0.8 * nrow(X))
X_train <- X[1:trainSize, ]
X_test <- X[(trainSize + 1):nrow(X), ]
y_train <- y[1:trainSize]
y_test <- y[(trainSize + 1):length(y)]
# Create model
data_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
params <- list(
objective = "reg:squarederror",
eval_metric = "rmse"
)
xgboost_model <- xgb.train(params = params, data = data_train, nrounds = 100)
# Create a data frame with the feature data used for training
feature_data <- as.data.frame(X_train)
# Create a prediction function that converts input data frames to DMatrix and predicts with the model
predict_xgb <- function(model, newdata) {
dmatrix <- xgb.DMatrix(as.matrix(newdata))
predict(model, dmatrix)
}
# Create the Predictor object using this prediction function
predictor <- Predictor$new(
model = xgboost_model,
data = feature_data,
y = y_train,
predict.fun = function(x) predict_xgb(xgboost_model, x)
)
sample_index <- 1
sample_instance <- feature_data[sample_index, , drop = FALSE]  # Ensure it's still a data frame
# Compute Shapley values for the selected instance
shapley <- Shapley$new(predictor = predictor, x.interest = sample_instance)
knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(xgboost)
library(dplyr)
library(SHAPforxgboost)
library(iml)
# load data collected from twelvedata
stock_data <- read.csv("../Data Collection/out.csv")
# Feature Engineering: Calculate technical indicators
stock_data$SMA <- SMA(Cl(stock_data), n = 20)  # 20-day Simple Moving Average
stock_data$RSI <- RSI(Cl(stock_data), n = 14)  # 14-day Relative Strength Index
# Prepare dataset for modeling
stock_data <- na.omit(stock_data)
X <- stock_data[, 2:ncol(stock_data)]
y <- lead(Cl(stock_data), default = NA)  # Lagged dependent variable (next day's closing price)
# Split data into training and testing sets
trainSize <- floor(0.8 * nrow(X))
X_train <- X[1:trainSize, ]
X_test <- X[(trainSize + 1):nrow(X), ]
y_train <- y[1:trainSize]
y_test <- y[(trainSize + 1):length(y)]
# Create model
data_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
params <- list(
objective = "reg:squarederror",
eval_metric = "mae"
)
xgboost_model <- xgb.train(params = params, data = data_train, nrounds = 100)
# Create a data frame with the feature data used for training
feature_data <- as.data.frame(X_train)
# Create a prediction function that converts input data frames to DMatrix and predicts with the model
predict_xgb <- function(model, newdata) {
dmatrix <- xgb.DMatrix(as.matrix(newdata))
predict(model, dmatrix)
}
# Create the Predictor object using this prediction function
predictor <- Predictor$new(
model = xgboost_model,
data = feature_data,
y = y_train,
predict.fun = function(x) predict_xgb(xgboost_model, x)
)
sample_index <- 1
sample_instance <- feature_data[sample_index, , drop = FALSE]  # Ensure it's still a data frame
# Compute Shapley values for the selected instance
shapley <- Shapley$new(predictor = predictor, x.interest = sample_instance)
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
# predict test data
dtest <- xgb.DMatrix(data = as.matrix(X_test))
y_pred <- predict(xgboost_model, dtest)
mae <- mean(na.omit(abs(y_pred - y_test)))
cat("This XGBoost model yields an MAE of:", mae)
knitr::opts_chunk$set(echo = TRUE)
library(quantmod)
library(xgboost)
library(dplyr)
library(SHAPforxgboost)
library(iml)
# load data collected from twelvedata
stock_data <- read.csv("../Data Collection/out.csv")
# Feature Engineering: Calculate technical indicators
stock_data$SMA <- SMA(Cl(stock_data), n = 20)  # 20-day Simple Moving Average
stock_data$RSI <- RSI(Cl(stock_data), n = 14)  # 14-day Relative Strength Index
# Prepare dataset for modeling
stock_data <- na.omit(stock_data)
X <- stock_data[, 2:ncol(stock_data)]
y <- lead(Cl(stock_data), default = NA)  # Lagged dependent variable (next day's closing price)
# Split data into training and testing sets
trainSize <- floor(0.8 * nrow(X))
X_train <- X[1:trainSize, ]
X_test <- X[(trainSize + 1):nrow(X), ]
y_train <- y[1:trainSize]
y_test <- y[(trainSize + 1):length(y)]
# Create model
data_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
params <- list(
objective = "reg:squarederror",
eval_metric = "mae"
)
xgboost_model <- xgb.train(params = params, data = data_train, nrounds = 100)
# Create a data frame with the feature data used for training
feature_data <- as.data.frame(X_train)
# Create a prediction function that converts input data frames to DMatrix and predicts with the model
predict_xgb <- function(model, newdata) {
dmatrix <- xgb.DMatrix(as.matrix(newdata))
predict(model, dmatrix)
}
# Create the Predictor object using this prediction function
predictor <- Predictor$new(
model = xgboost_model,
data = feature_data,
y = y_train,
predict.fun = function(x) predict_xgb(xgboost_model, x)
)
sample_index <- 1
sample_instance <- feature_data[sample_index, , drop = FALSE]  # Ensure it's still a data frame
# Compute Shapley values for the selected instance
shapley <- Shapley$new(predictor = predictor, x.interest = sample_instance)
# Plot residuals over time
residuals <- y_test - y_pred
plot(residuals, type = "h", col = "blue", main = "Residuals", xlab = "Time Index", ylab = "Residuals")
abline(h = 0, col = "red")  # Add a baseline at zero to highlight errors
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap_values
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
View(shap_values)
View(stock_data)
View(X_train)
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X))
shap_values
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X))
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap_values
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap_values$mean_shap_score
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
cat("Mean SHAP Score is:", shap_values$mean_shap_score)
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
# Calculate and visualize SHAP score for each feature
shap_values <- shap.values(xgb_model = xgboost_model, X_train = as.matrix(X_train))
shap_values$mean_shap_score
shap.plot.summary.wrap2(shap_values$shap_score, as.matrix(X_train))
